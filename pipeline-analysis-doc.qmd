---
title: "exome-seq-pipeline-analysis"
format: html
editor: visual
---

```{r}
trace_path <- "E:/Bioinformatics/Dissertation Stuff/sarek-log-files/execution_trace_single_sample_ERR166317.txt"
```

```{r}
library(tidyverse)
library(lubridate)
library(readr)
library(dplyr)
library(janitor)
library(scales)
library(DescTools)
library(knitr)
library(purrr)
library(lubridate)
```

```{r}
sarek_single_sample_trace <- read_tsv(
  "E:/Bioinformatics/Dissertation Stuff/sarek-log-files/execution_trace_single_sample_ERR166317.txt"
)
sarek_multi_sample_trace <- read_tsv("E:/Bioinformatics/Dissertation Stuff/sarek-log-files/execution_trace_multi_sample.txt")
  
sarek_large_sample_trace <- read_tsv("E:/Bioinformatics/Dissertation Stuff/sarek-log-files/execution_trace_large_sample.txt")


custom_pipeline_single_sample_trace <- read_tsv("E:/Bioinformatics/Dissertation Stuff/custom-pipeline-logs/single-sample-trace.txt")

custom_pipeline_multi_sample_trace <- read_tsv("E:/Bioinformatics/Dissertation Stuff/custom-pipeline-logs/multi-sample-trace.txt")

custom_pipeline_large_sample_trace <- read_tsv("E:/Bioinformatics/Dissertation Stuff/custom-pipeline-logs/large-multi-sample-trace.txt")
```

```{r}
parse_size_gb <- function(x) {
  if (is.numeric(x)) return(as.numeric(x) / (1024^3))
  y <- toupper(gsub("\\s+", "", as.character(x)))
  num  <- readr::parse_number(y)
  unit <- gsub("^[0-9.]+", "", y)
  dplyr::case_when(
    unit %in% c("", "B")  ~ num / (1024^3),
    unit %in% c("K","KB") ~ num / (1024^2),
    unit %in% c("M","MB") ~ num / 1024,
    unit %in% c("G","GB") ~ num,
    unit %in% c("T","TB") ~ num * 1024,
    TRUE ~ NA_real_
  )
}
```

```{r}

parse_dur_secs <- function(x) {
  s <- tolower(trimws(as.character(x)))
  ifelse(
    is.na(s) | s == "",
    NA_real_,
    {
      d <- as.numeric(stringr::str_extract(s, "(?<=\\b)\\d+(?=d)")); d[is.na(d)] <- 0
      h <- as.numeric(stringr::str_extract(s, "(?<=\\b)\\d+(?=h)")); h[is.na(h)] <- 0
      m <- as.numeric(stringr::str_extract(s, "(?<=\\b)\\d+(?=m)")); m[is.na(m)] <- 0
      sec <- as.numeric(stringr::str_extract(s, "(?<=\\b)\\d+(?=s)")); sec[is.na(sec)] <- 0
      d*86400 + h*3600 + m*60 + sec
    }
  )
}
```

```{r}
parse_cpu_percent_to_cores <- function(x) {
  v <- suppressWarnings(readr::parse_number(as.character(x)))
  v / 100
}
```

```{r}
# e.g. "%cpu" -> "percent_cpu"
custom_single_trace  <- custom_pipeline_single_sample_trace %>% clean_names() 
custom_multi_trace  <- custom_pipeline_multi_sample_trace %>% clean_names()
custom_large_trace  <- custom_pipeline_large_sample_trace %>% clean_names()
sarek_single_trace <- sarek_single_sample_trace %>% clean_names() 
sarek_multi_trace <- sarek_multi_sample_trace %>% clean_names() 
sarek_large_trace <- sarek_large_sample_trace %>% clean_names() 
# Safety: support either cleaned "percent_cpu" or raw `%cpu`
percent_col <- if ("percent_cpu" %in% names(custom_single_trace)) "percent_cpu" else "%cpu"
percent_col <- if ("percent_cpu" %in% names(custom_multi_trace)) "percent_cpu" else "%cpu"
percent_col <- if ("percent_cpu" %in% names(custom_large_trace)) "percent_cpu" else "%cpu"
percent_col <- if ("percent_cpu" %in% names(sarek_single_trace)) "percent_cpu" else "%cpu"
percent_col <- if ("percent_cpu" %in% names(sarek_multi_trace)) "percent_cpu" else "%cpu"
percent_col <- if ("percent_cpu" %in% names(sarek_large_trace)) "percent_cpu" else "%cpu"

```

Parse traces

```{r}
# Parse a single trace df and add computed fields
parse_one_trace <- function(df, percent_col = "%cpu") {
  # Safe extractors (return NA if column missing)
  col_or_na <- function(d, nm) if (nm %in% names(d)) d[[nm]] else NA

  # Times
  rtime   <- col_or_na(df, "realtime")
  dur     <- col_or_na(df, "duration")
  wall_sec <- dplyr::coalesce(parse_dur_secs(rtime), parse_dur_secs(dur))
  wall_hr  <- wall_sec / 3600

  # Sizes (observed)
  peak_rss_gb  <- parse_size_gb(col_or_na(df, "peak_rss"))
  peak_vmem_gb <- parse_size_gb(col_or_na(df, "peak_vmem"))
  rchar_gb     <- parse_size_gb(col_or_na(df, "rchar"))
  wchar_gb     <- parse_size_gb(col_or_na(df, "wchar"))

  # Cores from %cpu
  pct_vec   <- if (percent_col %in% names(df)) df[[percent_col]] else NA
  cores_avg <- parse_cpu_percent_to_cores(pct_vec)

  # Process name (strip " (TAG)")
  name_col <- col_or_na(df, "name")
  process  <- if (all(is.na(name_col))) NA_character_ else sub(" \\(.*\\)$", "", name_col)

  # Bind back
  df %>%
    mutate(
      wall_sec      = wall_sec,
      wall_hr       = wall_hr,
      peak_rss_gb   = peak_rss_gb,
      peak_vmem_gb  = peak_vmem_gb,
      rchar_gb      = rchar_gb,
      wchar_gb      = wchar_gb,
      cores_avg     = cores_avg,
      cpu_hours     = cores_avg * wall_hr,
      process       = process
    )
}

```

```{r}
parse_traces <- function(tr_list,
                         percent_col = "%cpu",
                         id_col = "run_id",
                         name_regex = "^(Custom|Sarek)_N(\\d+)$",
                         add_parsed_ids = TRUE) {
  stopifnot(is.list(tr_list), length(tr_list) > 0)

  parsed <- imap(tr_list, ~ parse_one_trace(.x, percent_col) %>%
                   mutate(!!id_col := .y)) %>%
            list_rbind()  # bind_rows but faster for lists

  if (add_parsed_ids && !is.null(names(tr_list))) {
    # Parse pipeline / n_samples from the id column using the regex
    m <- stringr::str_match(parsed[[id_col]], name_regex)
    if (!all(is.na(m))) {
      parsed <- parsed %>%
        mutate(
          pipeline  = if (ncol(m) >= 2) m[,2] else NA_character_,
          n_samples = if (ncol(m) >= 3) suppressWarnings(as.integer(m[,3])) else NA_integer_
        )
    }
  }

  parsed
}
```

```{r}
traces <- list(
  Custom_N1 = custom_single_trace,
  Custom_N3 = custom_multi_trace,
  Custom_N5 = custom_large_trace,
  Sarek_N1 = sarek_single_trace,
  Sarek_N3 = sarek_multi_trace,
  Sarek_N5 = sarek_large_trace
)

```

```{r}
parsed_all_traces <- parse_traces(traces, percent_col = "percent_cpu")

```

```{r}
library(dplyr)
library(purrr)
library(stringr)
library(lubridate)
library(ggplot2)

# --- Very robust duration parser (vectorised) ---
parse_realtime_secs <- function(x) {
  if (is.numeric(x)) return(as.numeric(x))
  x <- as.character(x)

  # Fast path: pure number -> seconds
  pure_num <- suppressWarnings(!is.na(as.numeric(x)))
  out <- rep(NA_real_, length(x))
  out[pure_num] <- as.numeric(x[pure_num])

  # Normalise text for parsing
  y <- x
  y <- str_trim(y)
  y <- str_replace_all(y, "\\[|\\]", "")           # strip brackets like [00:01:02]
  y <- str_replace_all(y, ",", ".")                # decimal commas
  y <- str_replace_all(y, "\\s+", "")              # remove spaces for unit-forms

  # 1) ISO-8601 durations: PT#H#M#S
  iso <- str_detect(y, "(?i)^P(T|$)")
  if (any(iso)) {
    # Try lubridate::as.duration
    suppressWarnings({
      dur <- as.duration(y[iso])
    })
    # as.duration on invalid strings gives NA seconds
    out[iso] <- as.numeric(dur)
  }

  # 2) HH:MM:SS(.fff) or MM:SS(.fff) (optionally with leading days like D-HH:MM:SS)
  # Extract optional day prefix
  has_colon <- str_detect(y, ":")
  if (any(has_colon & is.na(out))) {
    y2 <- y[has_colon & is.na(out)]

    # Handle D-HH:MM:SS by splitting off D-
    day_part <- str_match(y2, "^(\\d+)-")[,2]
    days_sec <- ifelse(is.na(day_part), 0, as.numeric(day_part) * 86400)
    timestr  <- str_replace(y2, "^\\d+-", "")

    # hms handles H:M:S, M:S, and fractional seconds
    secs <- suppressWarnings(period_to_seconds(hms(timestr)))
    secs[is.na(secs)] <- 0
    out[has_colon & is.na(out)] <- days_sec + secs
  }

  # 3) Unit strings like 1h2m3.45s, 2h, 45s, 1m2s345ms (any order)
  need_units <- is.na(out)
  if (any(need_units)) {
    z <- y[need_units]

    get_num <- function(pat) {
      v <- str_match(z, pat)[,2]
      as.numeric(ifelse(is.na(v), 0, v))
    }

    hrs <- get_num("(?i)(\\d+(?:\\.\\d+)?)h")
    min <- get_num("(?i)(\\d+(?:\\.\\d+)?)m(?!s)")  # 'm' not followed by 's'
    sec <- get_num("(?i)(\\d+(?:\\.\\d+)?)s(?![a-z])")
    ms  <- get_num("(?i)(\\d+(?:\\.\\d+)?)ms")

    out[need_units] <- hrs*3600 + min*60 + sec + ms/1000
  }

  # Any remaining NAs (unparseable) -> 0
  out[is.na(out)] <- 0
  out
}

# --- Recompute totals (uses your existing `traces` list) ---
totals <- purrr::imap_dfr(traces, ~{
  df <- .x
  stopifnot("realtime" %in% names(df))
  tibble(
    run = .y,
    total_sec = sum(parse_realtime_secs(df$realtime), na.rm = TRUE)
  )
}) %>%
  mutate(
    run = str_replace_all(run, "\\s+", ""),
    Pipeline = str_to_title(str_extract(run, "^[^_]+")),
    N = as.integer(str_extract(run, "(?<=_N)\\d+")),
    total_hr = total_sec / 3600
  ) %>%
  arrange(Pipeline, N)

# --- Quick sanity check (optional) ---
# print(totals)

# --- Plot ---
p <- ggplot(totals, aes(x = factor(N, levels = c(1,3,5)),
                        y = total_hr, fill = Pipeline)) +
  geom_col(position = position_dodge(width = 0.65), width = 0.6) +
  geom_text(aes(label = sprintf("%.1f h", total_hr)),
            position = position_dodge(width = 0.65),
            vjust = -0.3, size = 3) +
  scale_y_continuous("Total runtime (hours)",
                     expand = expansion(mult = c(0, 0.1))) +
  labs(
    title = "Total runtime per pipeline run",
    subtitle = "Custom vs nf-core/sarek across 1, 3, and 5 samples",
    x = "Number of samples (N)",
    fill = "Pipeline"
  ) +
  theme_minimal(base_size = 12)  + scale_fill_grey(start = 0.9, end = 0.3)

print(p)

```

```{r}
summarise_resource_totals <- function(parsed_df,
                                     include_run_id = TRUE,
                                     id_col = "run_id") {
  # choose grouping columns
  group_cols <- c("pipeline", "n_samples")
  if (include_run_id && id_col %in% names(parsed_df)) group_cols <- c(group_cols, id_col)

  totals <- parsed_df %>%
    summarise(
      `CPU-hours`       = sum(cores_avg   * wall_hr,    na.rm = TRUE),
      `Memory GB-hours` = sum(peak_rss_gb * wall_hr,    na.rm = TRUE),
      `Read GB`         = sum(rchar_gb,                 na.rm = TRUE),
      `Write GB`        = sum(wchar_gb,                 na.rm = TRUE),
      .by = all_of(group_cols)
    ) %>%
    pivot_longer(-all_of(group_cols), names_to = "metric", values_to = "value")

  totals
}
```

```{r}
totals_per_run <- summarise_resource_totals(parsed_all_traces, include_run_id = TRUE)
totals_per_run
```

```{r}
totals_long <- totals_per_run %>%
  mutate(
    metric = factor(metric, levels = c("CPU-hours","Memory GB-hours","Read GB","Write GB")),
    pipeline = factor(pipeline, levels = c("Custom","Sarek"))
  )
```

```{r}
p <- ggplot(totals_long, aes(x = factor(n_samples), y = value, fill = pipeline)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.6) +
  facet_wrap(~ metric, scales = "free_y") +
  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +
  labs(title = "Per-run resource totals by pipeline",
       x = "Number of samples (run size)",
       y = NULL,
       fill = NULL) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "top") + scale_fill_grey(start = 0.9, end = 0.3)
p

```

```{r}
models <- totals_long %>%
  group_by(pipeline, metric) %>%
  group_map(~ {
    fit <- lm(value ~ n_samples, data = .x)
    tibble(
      pipeline = .y$pipeline,
      metric   = .y$metric,
      R2       = summary(fit)$r.squared
    )
  }) %>%
  bind_rows()
```

```{r}
print(models)

```

```{r}
# totals_long must have: pipeline, n_samples, metric, value
df <- totals_long %>%
  mutate(
    metric    = factor(metric, levels = c("CPU-hours","Memory GB-hours","Read GB","Write GB")),
    n_samples = as.numeric(n_samples),
    pipeline  = factor(pipeline, levels = c("Custom","Sarek"))
  )

#R² per pipeline × metric
r2_tbl <- df %>%
  group_by(pipeline, metric) %>%
  summarise(R2 = summary(lm(value ~ n_samples, data = cur_data()))$r.squared,
            .groups = "drop") %>%
  mutate(label = paste0("R²=", sprintf("%.2f", R2)))

# Label positions
facet_stats <- df %>%
  group_by(metric) %>%
  summarise(
    x_max   = max(n_samples, na.rm = TRUE),
    y_max   = max(value, na.rm = TRUE),
    y_min   = min(value, na.rm = TRUE),
    y_range = pmax(y_max - y_min, 1e-8),  
    .groups = "drop"
  )

# offset each pipeline’s label vertically
r2_annot <- r2_tbl %>%
  left_join(facet_stats, by = "metric") %>%
  group_by(metric) %>%
  arrange(pipeline, .by_group = TRUE) %>%
  mutate(
    x = x_max + 0.05 * x_max,                   
    y = y_max - (row_number() - 1) * 0.10*y_range  # 10% of range between labels
  ) %>%
  ungroup()

# --- Plot
p <- ggplot(df, aes(x = n_samples, y = value, colour = pipeline)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  # R² labels 
  geom_text(
    data = r2_annot,
    aes(x = x, y = y, label = label, colour = pipeline),
    inherit.aes = FALSE,
    hjust = 0,            
    size = 4
  ) +
  facet_wrap(~ metric, scales = "free_y") +
  scale_color_grey(start = 0.2, end = 0.6) +   
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.12))) + 
  theme_minimal(base_size = 14) +
  theme(legend.position = "top") +
  labs(
    title  = "Scaling of Resource Usage with Sample Count",
    x      = "Number of Samples",
    y      = "Resource Usage",
    colour = "Pipeline"
  )

print(p)
ggsave("scaling_with_r2_greys.png", p, width = 12, height = 6, dpi = 300)
```

```{r}
plot_resource_bars <- function(totals_long,
                               per_sample = FALSE,
                               outfile = NULL,
                               width = 10, height = 6, dpi = 300) {
  df <- prep_totals_for_plot(totals_long) %>%
    mutate(yval = if (per_sample) value_mean / n_samples else value_mean,
           ylab = if (per_sample) "Per-sample value" else "Total")

  p <- ggplot(df, aes(x = factor(n_samples), y = yval, fill = pipeline)) +
    geom_col(position = position_dodge(width = 0.7), width = 0.6) +
    geom_errorbar(aes(ymin = pmax(0, yval - value_sd),
                      ymax = yval + value_sd),
                  position = position_dodge(width = 0.7),
                  width = 0.25, linewidth = 0.3) +
    facet_wrap(~ metric, scales = "free_y") +
    scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +
    labs(title = if (per_sample) "Per-sample resource usage by pipeline and cohort size"
                 else "Total resource usage by pipeline and cohort size",
         x = "Number of samples",
         y = unique(df$ylab),
         fill = NULL) +
    theme_minimal(base_size = 12) +
    theme(legend.position = "top") + scale_fill_grey(start = 0.9, end = 0.3)

  if (!is.null(outfile)) ggsave(outfile, p, width = width, height = height, dpi = dpi)
  p
}
```

```{r}

# per-sample
p_per_sample <- plot_resource_bars(totals_per_run, per_sample = TRUE,
                                   outfile = "resource_per_sample_by_metric.png")
p_per_sample
```

### Plots

#### Wall-clock by process

```{r}
library(stringr)

tr2 %>%
  filter(!is.na(wall_hr)) %>%
  ggplot(aes(x = reorder(process, wall_hr, FUN = median, na.rm = TRUE), y = wall_hr)) +
  geom_boxplot(outlier.alpha = 0.35) +
  coord_flip() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 40)) +
  labs(x = "Process", y = "Wall time (hours)", title = "Task wall time by process") +
  theme_minimal(base_size = 12)

```

#### CPU-hours by process

```{r}
# CPU-hours by process
by_proc %>%
  slice_max(total_cpu_h, n = 20) %>%
  ggplot(aes(x = reorder(process, total_cpu_h), y = total_cpu_h)) +
  geom_col() +
  coord_flip() +
  scale_y_continuous("Total CPU-hours", labels = comma) +
  labs(x = "Process", title = "Top CPU-hour consumers") +
  theme_minimal(base_size = 12)
```

```{r}

# write volume by process
tr2 %>%
  group_by(process) %>%
  summarise(total_w_gb = sum(wchar_gb, na.rm = TRUE), .groups = "drop") %>%
  slice_max(total_w_gb, n = 20) %>%
  ggplot(aes(x = reorder(process, total_w_gb), y = total_w_gb)) +
  geom_col() +
  coord_flip() +
  scale_y_continuous("Total written (GB)", labels = comma) +
  labs(x = "Process", title = "Top I/O writers") +
  theme_minimal(base_size = 12)
```

```{r}
totals <- tibble(
  metric = c("CPU-hours", "Memory GB-hours", "Read GB", "Write GB"),
  value  = c(
    sum(tr2$cores_avg * tr2$wall_hr, na.rm = TRUE),         # CPU-hours
    sum(tr2$peak_rss_gb * tr2$wall_hr, na.rm = TRUE),       # Memory GB-hours
    sum(tr2$rchar_gb, na.rm = TRUE),                        # total read GB
    sum(tr2$wchar_gb, na.rm = TRUE)                         # total written GB
  )
)
```

```{r}
ggplot(totals, aes(x = metric, y = value)) +
  geom_col() +
  geom_text(aes(label = ifelse(metric %in% c("CPU-hours","Memory GB-hours"),
                               comma(round(value, 1)),
                               comma(round(value)))),
            vjust = -0.4, size = 3.5) +
  scale_y_continuous("", labels = comma, expand = expansion(mult = c(0, 0.1))) +
  labs(x = NULL, title = "Pipeline-wide resource usage") +
  theme_minimal(base_size = 12)
```

### Variant Comparisons

```{r}
df <- tribble(
  ~SampleID,  ~Custom_SNPs, ~Sarek_SNPs, ~Sarek_Indels, ~Sarek_Total, ~Shared_SNPs, ~CustomOnly, ~SarekOnlySNPs,
  "ERR166317", 189648,       200747,      18178,         218912,       179614,       10034,       21120,
  "ERR166323", 209672,       224586,      20377,         244954,       199934,       9738,        24643,
  "ERR166327", 153233,       161640,      15858,         177484,       145771,       7462,        15855,
  "ERR166329", 167480,       176782,      16711,         193480,       158887,       8593,        17822
)

df
```

```{r}
variants <- df %>%
  mutate(
    increase = ((Sarek_SNPs - Custom_SNPs) / Sarek_SNPs * 100)
  )

# Calculate different summaries
mean_increase         <- mean(variants$increase, na.rm = TRUE)
median_increase       <- median(variants$increase, na.rm = TRUE)
trimmed_mean_increase <- mean(variants$increase, trim = 0.2, na.rm = TRUE)
winsor_mean_increase  <- mean(Winsorize(variants$increase), na.rm = TRUE)

# Show table with summaries
variants %>%
  add_row(SampleID = "Mean",    increase = mean_increase) %>%
  add_row(SampleID = "Median",  increase = median_increase) %>%
  add_row(SampleID = "Trimmed", increase = trimmed_mean_increase) %>%
  add_row(SampleID = "Winsor",  increase = winsor_mean_increase) %>%
  select(SampleID, increase) %>%
  print()
```

```{r}
chk <- df %>%
  mutate(
    check_custom = Shared_SNPs + CustomOnly == Custom_SNPs,
    check_sarek  = Shared_SNPs + SarekOnlySNPs  == Sarek_SNPs
  )
print(chk %>% select(SampleID, check_custom, check_sarek))
```

```{r}
df_union <- df %>%
  transmute(
    SampleID,
    Union = Shared_SNPs + CustomOnly + SarekOnlySNPs,
    Shared_pct = 100 * Shared_SNPs / Union,
    CustomOnly_pct = 100 * CustomOnly / Union,
    SarekOnly_pct  = 100 * SarekOnlySNPs  / Union
  )
df_union
```

```{r}

```

```{r}
df_union_long <- df_union %>%
  pivot_longer(cols = ends_with("_pct"),
               names_to = "Category", values_to = "Percent") %>%
  mutate(Category = factor(Category,
                           levels = c("Shared_pct","CustomOnly_pct","SarekOnly_pct"),
                           labels = c("Shared SNPs","Custom-only SNPs","Sarek-only SNPs")))
```

```{r}
p_union <- ggplot(df_union_long, aes(x = SampleID, y = Percent, fill = Category)) +
  geom_col(width = 0.7, color = "grey20", linewidth = 0.2) +
  scale_y_continuous(labels = label_percent(scale = 1), limits = c(0,100), expand = expansion(mult = c(0, 0.02))) +
  labs(title = "SNP Overlap (Total SNPs)",
       x = "Sample", y = "Percent of total SNP sites", fill = NULL) +
  theme_minimal(base_size = 12) + scale_fill_grey(start = 0.9, end = 0.3) +
  theme(legend.position = "top")
ggsave("snp_overlap_union_stacked.png", p_union, width = 8, height = 4.5, dpi = 300)
p_union
```

```{r}
# Summary stats for union-normalised view
union_summary <- df_union_long %>%
  group_by(Category) %>%
  summarise(Mean = mean(Percent), SD = sd(Percent), Min = min(Percent), Max = max(Percent), .groups="drop")
write_csv(union_summary,  "snp_overlap_union_summary.csv")
union_summary
```

```{r}
df_comp <- df %>%
  transmute(
    SampleID,
    # Custom composition
    Custom_Shared_pct    = 100 * Shared_SNPs / Custom_SNPs,
    Custom_Unique_pct    = 100 * CustomOnly  / Custom_SNPs,
    # Sarek composition
    Sarek_Shared_pct     = 100 * Shared_SNPs / Sarek_SNPs,
    Sarek_Unique_pct     = 100 * SarekOnlySNPs   / Sarek_SNPs
  )
```

```{r}
# Long format for plotting two panels (Custom vs Sarek)
df_comp_long <- df_comp %>%
  pivot_longer(cols = -SampleID, names_to = "Metric", values_to = "Percent") %>%
  separate(Metric, into = c("Pipeline","Part","pct"), sep = "_") %>%
  select(-pct) %>%
  mutate(
    Pipeline = factor(Pipeline, levels = c("Custom","Sarek")),
    Part = factor(Part, levels = c("Shared","Unique"), labels = c("Shared","Pipeline-unique"))
  )
```

```{r}
p_comp <- ggplot(df_comp_long, aes(x = SampleID, y = Percent, fill = Part)) +
  geom_col(width = 0.7, color = "grey20", linewidth = 0.2) +
  facet_wrap(~ Pipeline, nrow = 1) +
  scale_y_continuous(labels = label_percent(scale = 1), limits = c(0,100), expand = expansion(mult = c(0, 0.02))) +
  labs(title = "Per-pipeline SNP composition (each panel sums to 100%)",
       x = "Sample", y = "Percent within pipeline", fill = NULL) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "top")
ggsave("snp_overlap_per_pipeline.png", p_comp, width = 9, height = 4.5, dpi = 300)

p_comp
```

```{r}
# Summary stats per pipeline
comp_summary <- df_comp_long %>%
  group_by(Pipeline, Part) %>%
  summarise(Mean = mean(Percent), SD = sd(Percent), Min = min(Percent), Max = max(Percent), .groups = "drop")
write_csv(comp_summary, "snp_overlap_per_pipeline_summary.csv")
print(comp_summary)
```
